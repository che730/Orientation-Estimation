{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Orientation Estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkWKrDzbtLp2x0qF9pveRl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/che730/Orientation-Estimation/blob/master/Orientation_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1WJw7zETvMM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import plotly.figure_factory as ff\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "from IPython.display import display\n",
        "from random import choice\n",
        "from random import randint\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6xPRBgbT4i0",
        "outputId": "6bfb983d-0632-4cc2-fb32-817d00c3024d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1u2iqoBgmRb",
        "outputId": "8024aa49-3515-4047-dc7a-50c2468bf927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVTIT5MRuKFK"
      },
      "source": [
        "!git clone https://github.com/mihaidusmanu/d2-net.git\n",
        "% cd /content/d2-net/hpatches_sequences/\n",
        "!bash ./download.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQxAgSUuRNYB"
      },
      "source": [
        "!tar xvzf /content/gdrive/My\\ Drive/OxfordRelease.tar.gz -C /content\n",
        "!tar xvzf /content/gdrive/My\\ Drive/Viewpoints.tar.gz -C /content\n",
        "!tar xvzf /content/gdrive/My\\ Drive/WebcamRelease.tar.gz -C /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH3vQn-XfPhv"
      },
      "source": [
        "theta_ranges = [(0, 15), (15, 45), (45, 90), (90, 180)]\n",
        "cur_theta_idx = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEe0hVS5DBD6"
      },
      "source": [
        "!if [ -d '/content/gdrive/My Drive/Patches' ]; then echo \"Directory already exist\" ; else mkdir '/content/gdrive/My Drive/Patches' && echo \"Directory created\"; fi\n",
        "!if [ -d '/content/gdrive/My Drive/RotatedPatches_15' ]; then echo \"Directory already exist\" ; else mkdir '/content/gdrive/My Drive/RotatedPatches_15' && echo \"Directory created\"; fi\n",
        "!if [ -d '/content/gdrive/My Drive/RotatedPatches_45' ]; then echo \"Directory already exist\" ; else mkdir '/content/gdrive/My Drive/RotatedPatches_45' && echo \"Directory created\"; fi\n",
        "!if [ -d '/content/gdrive/My Drive/RotatedPatches_90' ]; then echo \"Directory already exist\" ; else mkdir '/content/gdrive/My Drive/RotatedPatches_90' && echo \"Directory created\"; fi\n",
        "!if [ -d '/content/gdrive/My Drive/RotatedPatches_180' ]; then echo \"Directory already exist\" ; else mkdir '/content/gdrive/My Drive/RotatedPatches_180' && echo \"Directory created\"; fi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXkpoMmbQSam"
      },
      "source": [
        "image_keypoints_path = '/content/gdrive/My Drive/image_keypoints.npy'\n",
        "patches_folder_path = '/content/gdrive/My Drive/Patches/'\n",
        "rotated_patches_folder_path = '/content/gdrive/My Drive/RotatedPatches_'\n",
        "thetas_folder_path = '/content/gdrive/My Drive/thetas_'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9DPC9Kw4Hr-"
      },
      "source": [
        "def create_keypoints(nfeatures, save):\n",
        "  orb = cv2.ORB_create(nfeatures)\n",
        "  f = open('/content/d2-net/image_list_hpatches_sequences.txt', 'r')\n",
        "\n",
        "  image_keypoints = []\n",
        "\n",
        "  while True:\n",
        "    img_path = f.readline()\n",
        "    if not img_path:\n",
        "      break\n",
        "    \n",
        "    img_path = '/content/d2-net/' + img_path[:-1]\n",
        "\n",
        "    src = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    keypoint, _ = orb.detectAndCompute(src, None)\n",
        "    keypoints_xy = cv2.KeyPoint_convert(keypoint)\n",
        "\n",
        "    image_keypoints.append((img_path, keypoints_xy))\n",
        "\n",
        "  f.close()\n",
        "\n",
        "  if save:\n",
        "    # (image path, keypoints) 파일 저장\n",
        "    np_image_keypoints = np.array(image_keypoints)\n",
        "    np.save(image_keypoints_path, np_image_keypoints)\n",
        "\n",
        "  return image_keypoints\n",
        "\n",
        "def load_keypoints():\n",
        "  image_keypoints = np.load(image_keypoints_path, allow_pickle=True)\n",
        "\n",
        "  return image_keypoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHWVqUYe5bQQ"
      },
      "source": [
        "def create_patches(image_keypoints, save):\n",
        "  patches = []\n",
        "\n",
        "  idx = 0\n",
        "  for image_idx, (img_path, keypoints) in enumerate(image_keypoints):\n",
        "    if image_idx % 50 == 0:\n",
        "      print(image_idx)\n",
        "    cur_img = Image.open(img_path)\n",
        "\n",
        "    for x, y in keypoints:\n",
        "      tmp_img = cur_img.copy()\n",
        "      if x - 16 < 0 or y - 16 < 0 or x + 16 >= cur_img.width or y + 16 >= cur_img.height:\n",
        "          print(\"Invalid cropping range\")\n",
        "          break\n",
        "      patch = tmp_img.crop((x - 16, y - 16, x + 16, y + 16)) # left, upper, right, lower\n",
        "      if save:\n",
        "        patch.save(patches_folder_path + str(idx) + '.jpg')\n",
        "      patches.append(patch)\n",
        "\n",
        "      idx += 1\n",
        "\n",
        "  return patches\n",
        "\n",
        "def load_patches():\n",
        "  patches = []\n",
        "  idx = 0\n",
        "  while True:\n",
        "    image_path = Path(patches_folder_path + str(idx) + '.jpg')\n",
        "    if not image_path.is_file():\n",
        "      break\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    patches.append(image)\n",
        "\n",
        "    idx += 1\n",
        "    \n",
        "  return patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKpNwdfe4Y51"
      },
      "source": [
        "def create_rotated_patches_thetas(theta_range, image_keypoints, save):\n",
        "  theta_min, theta_max = theta_range\n",
        "\n",
        "  rotated_patches = []\n",
        "  thetas = []\n",
        "\n",
        "  idx = 0\n",
        "  for image_idx, (img_path, keypoints) in enumerate(image_keypoints):\n",
        "    if image_idx % 50 == 0:\n",
        "      print(image_idx)\n",
        "    cur_img = Image.open(img_path)\n",
        "\n",
        "    for x, y in keypoints:\n",
        "      if theta_min == 0:\n",
        "        theta = randint(-theta_max, theta_max)\n",
        "      else:\n",
        "        theta = choice([randint(theta_min + 1,theta_max),randint(-theta_max, -theta_min - 1)])\n",
        "      thetas.append(theta)\n",
        "\n",
        "      tmp_img = cur_img.copy()\n",
        "      rotated_patch = tmp_img.rotate(theta, center=(x, y)).crop((x - 16, y - 16, x + 16, y + 16))\n",
        "      if save:\n",
        "        rotated_patch.save(rotated_patches_folder_path + str(theta_max) + '/' + str(idx) + '.jpg')\n",
        "      rotated_patches.append(rotated_patch)\n",
        "\n",
        "      idx += 1\n",
        "\n",
        "  if save:\n",
        "    np_thetas = np.array(thetas)\n",
        "    np.save(thetas_folder_path + str(theta_max) + '.npy', np_thetas)\n",
        "    # np_theta = np.load('/content/gdrive/My Drive/thetas_' + str(theta_max))\n",
        "\n",
        "  return rotated_patches, thetas\n",
        "\n",
        "def load_rotated_patches(theta_range):\n",
        "  _, theta_max = theta_range\n",
        "\n",
        "  rotated_patches = []\n",
        "  idx = 0\n",
        "  while True:\n",
        "    image_path = Path(rotated_patches_folder_path + str(theta_max) + '/' + str(idx) + '.jpg')\n",
        "    if not image_path.is_file():\n",
        "      break\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    rotated_patches.append(image)\n",
        "\n",
        "    idx += 1\n",
        "    \n",
        "  return rotated_patches\n",
        "\n",
        "def load_thetas(theta_range):\n",
        "  _, theta_max = theta_range\n",
        "  return np.load(thetas_folder_path + str(theta_max) + '.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmxSx-_LVg4_",
        "outputId": "9b649b66-63bb-4906-ed1a-c82c1f94278c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "image_keypoints_path = Path(image_keypoints_path)\n",
        "if image_keypoints_path.is_file():\n",
        "  print(\"image_keypoints already exists.\")\n",
        "else:\n",
        "  print(\"no image_keypoints file\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no image_keypoints file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LUTRBd8dgn-"
      },
      "source": [
        "Oxford_classes = ['bark', 'bikes', 'boat', 'graf', 'leuven', 'notredame', 'obama', 'paintedladies', 'rushmore', 'trees', 'ubc', 'wall', 'yosemite']\n",
        "Webcam_classes = ['Chamonix', 'Courbevoie', 'Frankfurt', 'Mexico', 'Panorama', 'StLouis']\n",
        "Viewpoints_classes = ['chatnoir', 'duckhunt', 'mario', 'outside', 'posters']\n",
        "\n",
        "test_dataset_names = ['OxfordRelease', 'Viewpoints', 'WebcamRelease']\n",
        "test_classes_per_datasets = [Oxford_classes, Viewpoints_classes, Webcam_classes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkNhHNNO_4Vz"
      },
      "source": [
        "def create_test_keypoints(nfeatures, test_dataset_name, test_classes):\n",
        "  print(\"test dataset:\", test_dataset_name)\n",
        "  orb = cv2.ORB_create(nfeatures)\n",
        "  image_keypoints = []\n",
        "\n",
        "  for test_class in test_classes:\n",
        "    print(\"test_class:\", test_class)\n",
        "    path_prefix = '/content/' + test_dataset_name + '/' + test_class + '/test/'\n",
        "    file_path = path_prefix + 'test_imgs.txt'\n",
        "    f = open(file_path, 'r')\n",
        "\n",
        "    while True:\n",
        "      img_path = f.readline()\n",
        "      if not img_path:\n",
        "        break\n",
        "      \n",
        "      img_path = path_prefix + 'image_color/' + img_path[11:-1]\n",
        "\n",
        "      src = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "      keypoint, _ = orb.detectAndCompute(src, None)\n",
        "      keypoints_xy = cv2.KeyPoint_convert(keypoint)\n",
        "\n",
        "      image_keypoints.append((img_path, keypoints_xy))\n",
        "\n",
        "    f.close()\n",
        "\n",
        "  print()\n",
        "\n",
        "  return image_keypoints\n",
        "\n",
        "def save_test_keypoints(test_image_keypoints, test_dataset_name):\n",
        "  # (image path, keypoints) 파일 저장\n",
        "  np_test_image_keypoints = np.array(test_image_keypoints)\n",
        "  np.save('/content/gdrive/My Drive/' + test_dataset_name + '/test_image_keypoints.npy', np_test_image_keypoints)\n",
        "\n",
        "\n",
        "def load_test_keypoints(test_dataset_name):\n",
        "  test_image_keypoints = np.load('/content/gdrive/My Drive/' + test_dataset_name + '/test_image_keypoints.npy', allow_pickle=True)\n",
        "\n",
        "  return test_image_keypoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTH1t3KQ_msW"
      },
      "source": [
        "def create_test_patches(image_keypoints, test_dataset_name, save):\n",
        "  test_patches = []\n",
        "  idx = 0\n",
        "\n",
        "  for image_path, keypoints in image_keypoints:\n",
        "    cur_img = Image.open(image_path)\n",
        "\n",
        "    for x, y in keypoints:\n",
        "      tmp_img = cur_img.copy()\n",
        "      if x - 16 < 0 or y - 16 < 0 or x + 16 >= cur_img.width or y + 16 >= cur_img.height:\n",
        "          print(\"Invalid cropping range\")\n",
        "          break\n",
        "      patch = tmp_img.crop((x - 16, y - 16, x + 16, y + 16)) # left, upper, right, lower\n",
        "      if save:\n",
        "        patch.save('/content/gdrive/My Drive/' + test_dataset_name + '/TestPatches/' + str(idx) + '.jpg')\n",
        "      test_patches.append(patch)\n",
        "      \n",
        "      idx += 1\n",
        "\n",
        "  return test_patches\n",
        "\n",
        "def load_test_patches(test_dataset_name):\n",
        "  test_patches = []\n",
        "  idx = 0\n",
        "  while True:\n",
        "    image_path = Path('/content/gdrive/My Drive/' + test_dataset_name + '/TestPatches/' + str(idx) + '.jpg')\n",
        "    if not image_path.is_file():\n",
        "      break\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    test_patches.append(image)\n",
        "\n",
        "    idx += 1\n",
        "    \n",
        "  return test_patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt8W-Sw1_msZ"
      },
      "source": [
        "def create_test_rotated_patches_thetas(theta_range, image_keypoints, test_dataset_name, save):\n",
        "  theta_min, theta_max = theta_range\n",
        "  test_rotated_patches = []\n",
        "  test_thetas = []\n",
        "  idx = 0\n",
        "\n",
        "  for image_path, keypoints in image_keypoints:\n",
        "      cur_img = Image.open(image_path)\n",
        "\n",
        "      for x, y in keypoints:\n",
        "        if theta_min == 0:\n",
        "          theta = randint(-theta_max, theta_max)\n",
        "        else :\n",
        "          theta = choice([randint(theta_min + 1,theta_max),randint(-theta_max, -theta_min - 1)])\n",
        "        test_thetas.append(theta)\n",
        "\n",
        "        tmp_img = cur_img.copy()\n",
        "        rotated_patch = tmp_img.rotate(theta, center=(x, y)).crop((x - 16, y - 16, x + 16, y + 16))\n",
        "        if save:\n",
        "          rotated_patch.save('/content/gdrive/My Drive/' + test_dataset_name + '/TestRotatedPatches_' + str(theta_max) + '/' + str(idx) + '.jpg')\n",
        "        test_rotated_patches.append(rotated_patch)\n",
        "        \n",
        "        idx += 1\n",
        "  \n",
        "  if save:\n",
        "    np_test_thetas = np.array(test_thetas)\n",
        "    np.save('/content/gdrive/My Drive/' + test_dataset_name + '/test_thetas_' + str(theta_max) + '.npy', np_test_thetas)\n",
        "\n",
        "  return test_rotated_patches, test_thetas\n",
        "\n",
        "def load_test_rotated_patches(theta_range, test_dataset_name):\n",
        "  _, theta_max = theta_range\n",
        "\n",
        "  test_rotated_patches = []\n",
        "  idx = 0\n",
        "  while True:\n",
        "    image_path = Path('/content/gdrive/My Drive/' + test_dataset_name + '/TestRotatedPatches_' + str(theta_max) + '/' + str(idx) + '.jpg')\n",
        "    if not image_path.is_file():\n",
        "      break\n",
        "    \n",
        "    image = Image.open(image_path)\n",
        "    test_rotated_patches.append(image)\n",
        "\n",
        "    idx += 1\n",
        "    \n",
        "  return test_rotated_patches\n",
        "\n",
        "def load_test_thetas(theta_range, test_dataset_name):\n",
        "  _, theta_max = theta_range\n",
        "  return np.load('/content/gdrive/My Drive/' + test_dataset_name + '/test_thetas_' + str(theta_max) + '.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b1zmWIy2pbi"
      },
      "source": [
        "class CustomDataset(Dataset): \n",
        "  def __init__(self, patches, rotated_patches, thetas):\n",
        "    if len(patches) != len(thetas) or len(patches) != len(rotated_patches):\n",
        "      print(\"data length is different\")\n",
        "      return \n",
        "\n",
        "    self.patches = patches\n",
        "    self.rotated_patches = rotated_patches\n",
        "    self.thetas = thetas\n",
        "\n",
        "  # 총 데이터의 개수를 리턴\n",
        "  def __len__(self): \n",
        "    return len(self.patches)\n",
        "\n",
        "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
        "  def __getitem__(self, idx): \n",
        "    # patch = transforms.ToTensor()(self.patches[idx]).unsqueeze(0)\n",
        "    patch = transforms.ToTensor()(self.patches[idx])\n",
        "    # rotated_patch = transforms.ToTensor()(self.rotated_patches[idx]).unsqueeze(0)\n",
        "    rotated_patch = transforms.ToTensor()(self.rotated_patches[idx])\n",
        "    theta = self.thetas[idx]\n",
        "    return patch, rotated_patch, theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1rGJY-U3Gic"
      },
      "source": [
        "def create_dataloader(image_keypoints, cur_theta_idx, batch_size, save):\n",
        "  patches = create_patches(image_keypoints, save=save)\n",
        "  rotated_patches, thetas = create_rotated_patches_thetas(theta_ranges[cur_theta_idx], image_keypoints, save=save)\n",
        "\n",
        "  dataset = CustomDataset(patches, rotated_patches=rotated_patches, thetas=thetas)\n",
        "  dataloader = DataLoader(dataset, batch_size, shuffle=False)\n",
        "\n",
        "  return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGnhHP_VmmiB"
      },
      "source": [
        "def create_test_dataloader(cur_theta_idx, test_dataset_name, test_classes, batch_size, nfeatures, save):\n",
        "  test_image_keypoints = create_test_keypoints(nfeatures, test_dataset_name, test_classes)\n",
        "  test_patches = create_test_patches(test_image_keypoints, test_dataset_name, save=save)\n",
        "  test_rotated_patches, test_thetas = create_test_rotated_patches_thetas(theta_ranges[cur_theta_idx], test_image_keypoints, test_dataset_name, save=save)\n",
        "\n",
        "  test_dataset = CustomDataset(test_patches, test_rotated_patches, test_thetas)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
        "\n",
        "  return test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBdq9ITk2MOz"
      },
      "source": [
        "# (image path, keypoints) 생성\n",
        "image_keypoints = create_keypoints(1000, save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UNK6m4rTx1z"
      },
      "source": [
        "patches = create_patches(image_keypoints, save=True)\n",
        "print(len(patches))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABzEsbgqpt6a"
      },
      "source": [
        "rotated_patches, thetas = create_rotated_patches_thetas(theta_ranges[2], image_keypoints, save=True)\n",
        "print(len(rotated_patches))\n",
        "print(len(thetas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXItugbMxhFI",
        "outputId": "94b6ee39-bc03-45da-c48f-689381735c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# (image path, keypoint) 파일 읽어오기\n",
        "image_keypoints = load_keypoints()\n",
        "print(len(image_keypoints))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8bRTL3h24rp"
      },
      "source": [
        "dataloader_theta_range_0 = create_dataloader(image_keypoints, theta_ranges[cur_theta_idx], batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYA8XwjM4P2s",
        "outputId": "359d0aca-415d-4079-b018-a93439089e79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_dataloaders_0 = []\n",
        "for i, (test_dataset_name, test_classes) in enumerate(list(zip(test_dataset_names, test_classes_per_datasets))):\n",
        "  if i == 0:\n",
        "    continue\n",
        "  test_dataloader = create_test_dataloader(theta_ranges[cur_theta_idx], test_dataset_name, test_classes, batch_size=32)\n",
        "  test_dataloaders_0.append(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test dataset: Viewpoints\n",
            "test_class: chatnoir\n",
            "test_class: duckhunt\n",
            "test_class: mario\n",
            "test_class: outside\n",
            "test_class: posters\n",
            "\n",
            "test dataset: WebcamRelease\n",
            "test_class: Chamonix\n",
            "test_class: Courbevoie\n",
            "test_class: Frankfurt\n",
            "test_class: Mexico\n",
            "test_class: Panorama\n",
            "test_class: StLouis\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCh-AOMM9n93"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, theta_idx):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "    self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "    self.conv5 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "    self.conv7 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "    self.conv8 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.bn4 = nn.BatchNorm2d(64)\n",
        "    self.bn5 = nn.BatchNorm2d(128)\n",
        "    self.bn6 = nn.BatchNorm2d(128)\n",
        "    self.bn7 = nn.BatchNorm2d(256)\n",
        "    self.bn8 = nn.BatchNorm2d(256)\n",
        "\n",
        "    self.maxPool = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "    self.fc1 = nn.Linear(2*2*256, 2*2*256)\n",
        "    self.fc2 = nn.Linear(2*2*256, 1)\n",
        "\n",
        "    self.metric = 0\n",
        "    self.optimizer = optim.SGD(self.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
        "    self.scheduler = ReduceLROnPlateau(self.optimizer, factor=0.1, patience=10, min_lr=1e-7)\n",
        "    self.epoch_num = 100\n",
        "\n",
        "    self.path = '/content/gdrive/My Drive/state_dict_' + str(theta_idx) + '.pt'\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = self.maxPool(x)\n",
        "\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    x = F.relu(self.bn4(self.conv4(x)))\n",
        "    x = self.maxPool(x)\n",
        "\n",
        "    x = F.relu(self.bn5(self.conv5(x)))\n",
        "    x = F.relu(self.bn6(self.conv6(x)))\n",
        "    x = self.maxPool(x)\n",
        "\n",
        "    x = F.relu(self.bn7(self.conv7(x)))\n",
        "    x = F.relu(self.bn8(self.conv8(x)))\n",
        "    x = self.maxPool(x)\n",
        "\n",
        "    x = x.view(-1, 2 * 2 * 256)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "  def criterion(self, prediction, rotated_prediction, theta):\n",
        "    batch_size = len(prediction)\n",
        "    zero = torch.zeros([batch_size, 1]).to(device)\n",
        "    Lr1 = torch.max(abs(prediction) - 180, zero)\n",
        "    Lr2 = torch.max(abs(rotated_prediction) - 180, zero)\n",
        "    diff = rotated_prediction - prediction\n",
        "    theta = torch.reshape(theta, [batch_size, 1])\n",
        "    Lp = torch.min(torch.abs(diff - theta), 2 * 180 - torch.abs(diff - theta))\n",
        "    L = Lr1 + Lr2 + 1e-1 * Lp\n",
        "\n",
        "    return L\n",
        "    \n",
        "  def resume(self):\n",
        "    checkpoint = torch.load(self.path)\n",
        "    self.load_state_dict(checkpoint['model_state_dict'])\n",
        "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    self.train_losses = checkpoint['loss']\n",
        "    self.train_errors = checkpoint['error']\n",
        "\n",
        "  def train(self, dataloader):\n",
        "    self.train_losses = []\n",
        "    self.train_errors = []\n",
        "\n",
        "    start_epoch = 0;\n",
        "\n",
        "    count = 0\n",
        "    for i in range(start_epoch, self.epoch_num):\n",
        "      count += 1\n",
        "      if i % 50:\n",
        "        torch.save({\n",
        "            'epoch': i,\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'loss': self.train_losses,\n",
        "            'error': self.train_errors\n",
        "            }, self.path)\n",
        "        \n",
        "      if count % 100 == 0:\n",
        "        print(count, end=\" \")\n",
        "\n",
        "      # train\n",
        "      error = 0\n",
        "      for patch, rotated_patch, theta in dataloader:\n",
        "        patch = patch.to(device)\n",
        "        rotated_patch = rotated_patch.to(device)\n",
        "        theta = theta.to(device)\n",
        "\n",
        "        prediction = self(patch)\n",
        "        rotated_prediction = self(rotated_patch)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss = self.criterion(prediction, rotated_prediction, theta)\n",
        "        loss = torch.mean(loss)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step(self.metric)\n",
        "\n",
        "        error = torch.mean(torch.abs(torch.abs(prediction - rotated_prediction) - theta))\n",
        "\n",
        "      self.train_losses.append(loss)\n",
        "      self.train_errors.append(error)\n",
        "    \n",
        "    torch.save({\n",
        "            'epoch': 0,\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'loss': self.train_losses,\n",
        "            'error': self.train_errors\n",
        "            }, self.path)\n",
        "  \n",
        "    print(\"train done.\")\n",
        "\n",
        "  def test(self, testloader):\n",
        "    test_losses = []\n",
        "    test_erorrs = []\n",
        "    with torch.no_grad():\n",
        "      for patch, rotated_patch, theta in testloader:\n",
        "        patch = patch.to(device)\n",
        "        rotated_patch = rotated_patch.to(device)\n",
        "        theta = theta.to(device)\n",
        "\n",
        "        prediction = self(patch)\n",
        "        rotated_prediction = self(rotated_patch)\n",
        "    \n",
        "        loss = self.criterion(prediction, rotated_prediction, theta)\n",
        "        test_losses.extend(loss)\n",
        "\n",
        "        error = torch.abs(torch.abs(prediction - rotated_prediction) - theta)\n",
        "        test_erorrs.extend(error)\n",
        "      \n",
        "    return test_losses\n",
        "    \n",
        "  def plot_training_curve(self):\n",
        "    plt.plot(range(1, len(self.train_losses) + 1), self.train_losses, label=\"train loss\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title('Train Loss per Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(range(1, len(self.train_errors) + 1), self.train_errors, label=\"train errors\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('error')\n",
        "    plt.title('Train Error per Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "  def plot_test_curve(self, test_losses, test_errors):\n",
        "    plt.plot(range(1, len(test_losses) + 1), test_losses, label=\"test loss\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title('Test Loss per Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(range(1, len(test_errors) + 1), test_errors, label=\"test error\")\n",
        "    plt.xlabel('epoch num')\n",
        "    plt.ylabel('error')\n",
        "    plt.title('Test Error per Epoch')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce_Tkgo4zOIj"
      },
      "source": [
        "# theta range 0\n",
        "net = Net(cur_theta_idx)\n",
        "net = net.to(device)\n",
        "net.train(dataloader, resume=False)\n",
        "net.plot_training_curve()\n",
        "print(\"Last train loss:\", net.train_losses[-1].item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sST7JPmly3U"
      },
      "source": [
        "# test\n",
        "net = Net(cur_theta_idx)\n",
        "net = net.to(device)\n",
        "net.resume()\n",
        "for test_dataloader in test_dataloaders_0:\n",
        "  print(\"test start\")\n",
        "  test_losses = net.test(test_dataloader)\n",
        "  net.plot_test_curve(test_losses)\n",
        "  print(\"Average test loss:\", np.mean([x.item() for x in test_losses]))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}